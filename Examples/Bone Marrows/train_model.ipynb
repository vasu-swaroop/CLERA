{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sindy_utils import library_size\n",
    "import tensorflow as tf\n",
    "from training import train_network\n",
    "from preprocess_utils import split_data, build_network_layers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Innitialize the parameter dictionary\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters used for defining the preprocessing over the dataset\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['window_size']=4#The window length of averaging\n",
    "params['stride']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"gene_names.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    name_genes = pickle.load(pkl_file)\n",
    "path=\"time_series.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    training_dict = pickle.load(pkl_file)\n",
    "path=\"preprocess_params.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    preprocess_params = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in preprocess_params.items():\n",
    "    params[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = split_data(training_dict, validation_ratio=0.1) #split the data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data['x'].shape, training_data['dx'].shape, training_data['classes'].shape) #verify the shapes of the training data\n",
    "print(validation_data['x'].shape, validation_data['dx'].shape,validation_data['classes'].shape) #verify the shapes of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=training_data['classes']\n",
    "if not isinstance(check, pd.DataFrame):\n",
    "    classes_df = pd.DataFrame(check)\n",
    "else:\n",
    "    classes_df = check\n",
    "\n",
    "# Sum along the rows to get the count of each class\n",
    "class_counts = classes_df.sum(axis=0)\n",
    "\n",
    "# Convert the result to a dictionary for easy access\n",
    "class_counts_dict = class_counts.to_dict()\n",
    "\n",
    "print(\"Number of elements of each class:\")\n",
    "print(class_counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=validation_data['classes']\n",
    "if not isinstance(check, pd.DataFrame):\n",
    "    classes_df = pd.DataFrame(check)\n",
    "else:\n",
    "    classes_df = check\n",
    "\n",
    "# Sum along the rows to get the count of each class\n",
    "class_counts = classes_df.sum(axis=0)\n",
    "\n",
    "# Convert the result to a dictionary for easy access\n",
    "class_counts_dict = class_counts.to_dict()\n",
    "\n",
    "print(\"Number of elements of each class:\")\n",
    "print(class_counts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target folder name to track the experiments\n",
    "params['folder']='test'\n",
    "\n",
    "latent_dim=6\n",
    "params['model']='Bone Marrows'\n",
    "params['input_dim'] = 2000\n",
    "params['latent_dim'] =6\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 2\n",
    "params['include_sine'] = True\n",
    "params['include_constant']=True\n",
    "#The below inclide parameters have only been simulated for model_order 1 for now\n",
    "if params['model_order']==1:\n",
    "    params['include_tan']=False\n",
    "    params['include_log']=False #this can be modified later such that the domain restirctions of log doesnt become a problem. For now it is kept to false.\n",
    "    params['include_exp']=True\n",
    "    params['include_reciprocal_func']=True\n",
    "else:\n",
    "    params['include_tan']=False\n",
    "    params['include_log']=False\n",
    "    params['include_exp']=True\n",
    "    params['include_reciprocal_func']=False\n",
    "params['library_dim'] = library_size(n=params['latent_dim'], poly_order=params['poly_order'], use_sine=params['include_sine'], include_constant=params['include_constant'],use_tan=params['include_tan'],use_log=params['include_log'],use_exp=params['include_exp'],use_reciprocal=params['include_reciprocal_func'])\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 200\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 10\n",
    "params['loss_weight_sindy_z'] = 1e-2\n",
    "params['loss_weight_sindy_x'] = 1e-1\n",
    "params['loss_weight_sindy_regularization'] =1e-2\n",
    "params['autoencoder_regularization']=1e-6\n",
    "\n",
    "params['weights']=[params['loss_weight_decoder'],params['loss_weight_sindy_z'],params['loss_weight_sindy_x'],params['loss_weight_sindy_regularization'],params['autoencoder_regularization'], params['latent_dim'],params['coefficient_threshold']]\n",
    "\n",
    "params['activation'] = 'relu'\n",
    "params['widths'] = [512,32]\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = training_data['x'].shape[0]\n",
    "params['learning_rate'] =1e-3\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 50\n",
    "\n",
    "params['num_classes']=8\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 1\n",
    "params['refinement_epochs'] =1\n",
    "params['terms']=4*latent_dim+6 # put as None if the training should go for entire epoch defined. If the number of active terms go below this, the training stops\n",
    "\n",
    "\n",
    "#classifier parameters\n",
    "params['classify']=True\n",
    "params['classifier_widths']=[8]\n",
    "params['weights']=params['weights']+[params['classifier_widths']]\n",
    "params['loss_class']=1\n",
    "num_instance = 4\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_instance):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "    params['encoder_weights']=build_network_layers(params['input_dim'], params['latent_dim'], params['widths'], 'encoder')\n",
    "    params['decoder_weights']=build_network_layers(params['latent_dim'], params['input_dim'], params['widths'][::-1], 'decoder')\n",
    "    params['classifier_weights']=build_network_layers( params['latent_dim'], params['num_classes'], params['classifier_widths'], 'encoder')\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "    \n",
    "    # This can be any relevant format\n",
    "    # params['folder'] = params['model']+str(params['widths'])+str(params['weights'])+str(params['learning_rate'])\n",
    "    params['save_name']=datetime.datetime.now().strftime(\"%Y_%m_%d_%H%f\")\n",
    "    params['coefficient_initialization'] = 'specified'\n",
    "\n",
    "    params['init_coefficients']=np.asarray(np.random.choice([-1, 1], size=(params['library_dim'], params['latent_dim'])), dtype=np.float32)\n",
    "\n",
    "    # Create a save folder if it doesn't exist\n",
    "    save_folder = os.path.join(params['data_path'], params['folder'])\n",
    "    params['save_folder']=save_folder+\"\\\\\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    os.chdir(save_folder)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Train the network and obtain results\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "# Change back to the original data path\n",
    "os.chdir(params['data_path'])    \n",
    "# Save the DataFrame to a pickle file with a timestamped filename\n",
    "df.to_pickle(f'experiment_results_{params[\"folder\"]}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
