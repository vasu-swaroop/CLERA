{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sindy_utils import library_size\n",
    "import tensorflow as tf\n",
    "from training import train_network\n",
    "from preprocess_utils import split_data, build_network_layers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Innitialize the parameter dictionary\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters used for defining the preprocessing over the dataset\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['window_size']=4#The window length of averaging\n",
    "params['stride']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"gene_names.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    name_genes = pickle.load(pkl_file)\n",
    "path=\"time_series.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    training_dict = pickle.load(pkl_file)\n",
    "path=\"preprocess_params.pkl\"\n",
    "with open(path, 'rb') as pkl_file:\n",
    "    preprocess_params = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = split_data(training_dict, validation_ratio=0.1) #split the data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2835, 100) (2835, 100) (2835, 9)\n",
      "(315, 100) (315, 100) (315, 9)\n"
     ]
    }
   ],
   "source": [
    "print(training_data['x'].shape, training_data['dx'].shape, training_data['classes'].shape) #verify the shapes of the training data\n",
    "print(validation_data['x'].shape, validation_data['dx'].shape,validation_data['classes'].shape) #verify the shapes of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements of each class:\n",
      "{0: 321, 1: 298, 2: 312, 3: 320, 4: 315, 5: 325, 6: 317, 7: 315, 8: 312}\n"
     ]
    }
   ],
   "source": [
    "check=training_data['classes']\n",
    "if not isinstance(check, pd.DataFrame):\n",
    "    classes_df = pd.DataFrame(check)\n",
    "else:\n",
    "    classes_df = check\n",
    "\n",
    "# Sum along the rows to get the count of each class\n",
    "class_counts = classes_df.sum(axis=0)\n",
    "\n",
    "# Convert the result to a dictionary for easy access\n",
    "class_counts_dict = class_counts.to_dict()\n",
    "\n",
    "print(\"Number of elements of each class:\")\n",
    "print(class_counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = pd.read_csv('sergio.csv')\n",
    "count_data = count_matrix.iloc[:,2:]\n",
    "count_data['Ordinal Tag']=count_matrix.iloc[:,1]\n",
    "# Separate cell type labels from count data\n",
    "cell_types=count_matrix['Ordinal Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements of each class:\n",
      "{0: 29, 1: 52, 2: 38, 3: 30, 4: 35, 5: 25, 6: 33, 7: 35, 8: 38}\n"
     ]
    }
   ],
   "source": [
    "check=validation_data['classes']\n",
    "if not isinstance(check, pd.DataFrame):\n",
    "    classes_df = pd.DataFrame(check)\n",
    "else:\n",
    "    classes_df = check\n",
    "\n",
    "# Sum along the rows to get the count of each class\n",
    "class_counts = classes_df.sum(axis=0)\n",
    "\n",
    "# Convert the result to a dictionary for easy access\n",
    "class_counts_dict = class_counts.to_dict()\n",
    "\n",
    "print(\"Number of elements of each class:\")\n",
    "print(class_counts_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "TRAINING\n",
      "Legend\n",
      "['Combined loss', 'Reconstruction loss', 'SINDy_z loss', 'SINDy_x loss', 'Sindy Regularisation- L1 Norm', 'Autoencoder weights- L1 Norm', 'Classification Loss']\n",
      "Epoch 0\n",
      "Training loss 1937.5587158203125, (33.83383, 319.36795, 2.85077, 0.9999276, 6318.869, 1.0326312)\n",
      "Validation loss 1980.326416015625, (33.379032, 328.83636, 2.9098032, 0.9999276, 6318.869, 1.0001167)\n",
      "decoder loss ratio: 0.995756, decoder SINDy loss  ratio: 76.991599, SINDy z loss ratio: 44252.925781\n",
      "REFINEMENT\n",
      "Epoch 0\n",
      "Training loss 2057.67333984375, (33.668495, 343.9715, 3.437169, 0.99984056, 6287.018, 0.7240681)\n",
      "Validation loss 1834.447021484375, (33.221977, 300.23105, 2.9839766, 0.99984056, 6287.018, 0.7108742)\n",
      "decoder loss ratio: 0.991070, decoder SINDy loss  ratio: 78.954183, SINDy z loss ratio: 40403.386719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a61888887735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# Train the network and obtain results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mresults_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\Desktop\\Model Discovery Code\\Method\\src\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# Save the trained model and parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'save_folder'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'save_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# Save the parameters to a pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1675\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m           self.export_meta_graph(\n\u001b[1;32m-> 1677\u001b[1;33m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1720\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1722\u001b[1;33m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1724\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m   2057\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[1;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m       \u001b[0msaver_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Vasu\\anaconda3\\envs\\sindy\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[1;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[0;32m    581\u001b[0m   \u001b[1;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m   \u001b[1;31m# pylint: disable=g-explicit-length-test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstripped_op_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m     meta_graph_def.meta_info_def.stripped_op_list.MergeFrom(\n\u001b[0;32m    585\u001b[0m         stripped_op_list_for_graph(meta_graph_def.graph_def))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Target folder name to track the experiments\n",
    "params['folder']='test'\n",
    "\n",
    "latent_dim=6\n",
    "params['model']='SERGIO'\n",
    "params['input_dim'] = 100\n",
    "params['latent_dim'] =6\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 2\n",
    "params['include_sine'] = True\n",
    "params['include_constant']=True\n",
    "#The below inclide parameters have only been simulated for model_order 1 for now\n",
    "if params['model_order']==1:\n",
    "    params['include_tan']=False\n",
    "    params['include_log']=False #this can be modified later such that the domain restirctions of log doesnt become a problem. For now it is kept to false.\n",
    "    params['include_exp']=True\n",
    "    params['include_reciprocal_func']=True\n",
    "else:\n",
    "    params['include_tan']=False\n",
    "    params['include_log']=False\n",
    "    params['include_exp']=True\n",
    "    params['include_reciprocal_func']=False\n",
    "params['library_dim'] = library_size(n=params['latent_dim'], poly_order=params['poly_order'], use_sine=params['include_sine'], include_constant=params['include_constant'],use_tan=params['include_tan'],use_log=params['include_log'],use_exp=params['include_exp'],use_reciprocal=params['include_reciprocal_func'])\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 200\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 10\n",
    "params['loss_weight_sindy_z'] = 5\n",
    "params['loss_weight_sindy_x'] = 0.1\n",
    "params['loss_weight_sindy_regularization'] =1\n",
    "params['autoencoder_regularization']=1e-5\n",
    "\n",
    "params['weights']=[params['loss_weight_decoder'],params['loss_weight_sindy_z'],params['loss_weight_sindy_x'],params['loss_weight_sindy_regularization'],params['autoencoder_regularization'], params['latent_dim'],params['coefficient_threshold']]\n",
    "\n",
    "params['activation'] = 'relu'\n",
    "params['widths'] = [512,32]\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = training_data['x'].shape[0]\n",
    "params['learning_rate'] =1e-3\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 50\n",
    "\n",
    "params['num_classes']=9\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 1\n",
    "params['refinement_epochs'] =1\n",
    "params['terms']=4*latent_dim+6 # put as None if the training should go for entire epoch defined. If the number of active terms go below this, the training stops\n",
    "\n",
    "\n",
    "#classifier parameters\n",
    "params['classify']=True\n",
    "params['classifier_widths']=[8]\n",
    "params['weights']=params['weights']+[params['classifier_widths']]\n",
    "params['loss_class']=1\n",
    "num_instance = 4\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_instance):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "    params['encoder_weights']=build_network_layers(params['input_dim'], params['latent_dim'], params['widths'], 'encoder')\n",
    "    params['decoder_weights']=build_network_layers(params['latent_dim'], params['input_dim'], params['widths'][::-1], 'decoder')\n",
    "    params['classifier_weights']=build_network_layers( params['latent_dim'], params['num_classes'], params['classifier_widths'], 'encoder')\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "    \n",
    "    # This can be any relevant format\n",
    "    # params['folder'] = params['model']+str(params['widths'])+str(params['weights'])+str(params['learning_rate'])\n",
    "    params['save_name']=datetime.datetime.now().strftime(\"%Y_%m_%d_%H%f\")\n",
    "    params['coefficient_initialization'] = 'specified'\n",
    "\n",
    "    params['init_coefficients']=np.asarray(np.random.choice([-1, 1], size=(params['library_dim'], params['latent_dim'])), dtype=np.float32)\n",
    "\n",
    "    # Create a save folder if it doesn't exist\n",
    "    save_folder = os.path.join(params['data_path'], params['folder'])\n",
    "    params['save_folder']=save_folder+\"\\\\\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    os.chdir(save_folder)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Train the network and obtain results\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "# Change back to the original data path\n",
    "os.chdir(params['data_path'])    \n",
    "# Save the DataFrame to a pickle file with a timestamped filename\n",
    "df.to_pickle(f'experiment_results_{params[\"folder\"]}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
